# Resume Analyzer & Skill Matcher

An AI-powered resume analysis and skill-matching service. Upload a PDF resume and compare it against job descriptions to get keyword matches, similarity scores, and missing skills. Built on AWS using Terraform with a scalable, serverless-friendly architecture.

---

## Features

- Upload PDF resumes for automatic parsing
- Match resumes against job descriptions using MiniLM + FAISS embeddings
- Calculates similarity score and keyword match percentage
- Highlights missing keywords
- Logs metadata to DynamoDB
- Monitors errors in CloudWatch Logs
- Sends SNS email alerts for â€œERRORâ€ entries in logs

---
## Tech Stack

- **Backend**: Python 3, Flask, Sentence Transformers (MiniLM), FAISS, PyMuPDF
- **Frontend**: Static HTML/CSS (Tailwind-based) hosted on S3
- **AI/ML**: MiniLM for embeddings, FAISS for vector similarity search
- **Cloud Infra**: AWS EC2, ALB, S3, DynamoDB, CloudWatch, SNS, IAM, VPC, NAT Gateway, AWS Gaurduty, SNS.
- **IaC**: Terraform
---

## Deployment Guide

### 1. **Clone the repo**
### 2. Edit for SNS alerts.
- In line 552 of main.tf file replace your mail id in the placeholder.
### 3. Prepare Your Files
Upload the following manually to your AWS CloudShell:
- `main.tf` â€“ your Terraform configuration file
- `resume-backend.zip` â€“ zipped Flask backend folder
- `index.html` or frontend files â€“ static output of your React app

### 4. Initialize and Deploy
```bash
terraform init
terraform apply
```
## 5. Access the Application

- **Frontend URL**: Shown as `frontend_s3_url` in Terraform output  
- **Backend**: Two EC2 instances behind an ALB (port 80), each running Flask on port `5000`

> Paste the frontend URL in your browser to begin.

---

## API Endpoint

### `POST /analyze` (handled by the Flask backend)

**Form Data:**
- `resume`: PDF file *(required)*
- `jobdesc`: Optional plain-text job description

**Sample Response:**
```json
{
  "similarity_score": 0.87,
  "keyword_match_percent": 76.9,
  "keywords_matched": 10,
  "missing_keywords": ["terraform", "aws", "docker"]
}
```
## ğŸ“ˆ Monitoring & Alerts

- **Logs**: Captured via `app.log` and `setup.log`, pushed to CloudWatch
- **Metric Filter**: Triggers if any `"ERROR"` appears in logs
- **SNS Alerts**: Email sent to your configured address for immediate issue notification
- **GuardDuty**: Active for threat detection (forwarded via EventBridge to SNS)

---

## ğŸ” Security & Permissions

- **IAM Role**: EC2 instances assume a custom role with scoped access to S3, DynamoDB, and CloudWatch
- **Private Subnets**: EC2s are deployed in private subnets with no public IPs
- **NAT Gateway**: Enables secure outbound internet access from private EC2s
- **Security Groups**:
  - EC2: Accepts port 5000 traffic only from ALB
  - ALB: Accepts HTTP traffic (port 80) from the internet

---

## ğŸ“Š Terraform Outputs

After successful deployment, you will see:

- âœ… `frontend_s3_url` â€“ for accessing the app UI  
- âœ… `ec2_public_ip` (ALB DNS Name) â€“ for backend API testing/debugging  
- âœ… DynamoDB table name â€“ where analysis logs are stored

---

## ğŸ§ª Testing Tips

- âœ… Upload a **resume only**  
- âœ… Upload a **resume + job description**
- âœ… Confirm that:
  - Similarity score is returned
  - Missing keywords are shown
  - DynamoDB and CloudWatch log your interaction
  - SNS email is triggered on errors

---

## ğŸ’° Cost Optimization

- EC2: Two `t3.medium` instances (balanced cost and compute)
- ALB: Single ALB in front of EC2s
- S3: `force_destroy = true` used for cleanup
- CloudWatch: Log retention set to 7 days
- DynamoDB: PAY_PER_REQUEST billing mode
- NAT Gateway: Single AZ usage to reduce cost

---

## ğŸ“š References

- [Sentence Transformers (MiniLM)](https://www.sbert.net/)
- [FAISS by Facebook](https://github.com/facebookresearch/faiss)
- [AWS CloudWatch Agent](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html)
- [Terraform AWS Provider](https://registry.terraform.io/providers/hashicorp/aws/latest/docs)
